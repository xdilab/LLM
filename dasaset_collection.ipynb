{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lN3LYU0X8EUn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from threading import Lock\n",
        "\n",
        "\n",
        "urls = [\n",
        "    \"https://null-byte.wonderhowto.com/how-to/enumerate-smb-with-enum4linux-smbclient-0198049/\",\n",
        "    \"https://null-byte.wonderhowto.com/how-to/exploit-eternalblue-windows-server-with-metasploit-0195413/\",\n",
        "    \"https://null-byte.wonderhowto.com/how-to/automate-wi-fi-hacking-with-wifite2-0191739/\",\n",
        "    ]\n",
        "\n",
        "lock = Lock()\n",
        "\n",
        "def scrape_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    data = []\n",
        "\n",
        "\n",
        "    sections = soup.find_all('h2', class_='step')\n",
        "    for section in sections:\n",
        "        section_title = section.text.strip()\n",
        "\n",
        "        prompt = None\n",
        "\n",
        "        steps = section.find_all_next(['p', 'pre'])\n",
        "        for index, step in enumerate(steps, start=1):\n",
        "            if step.name == 'p':\n",
        "                prompt = step.text.strip()\n",
        "            elif step.name == 'pre' and 'syntax-highlighted' in step.get('class', []):\n",
        "                response = step.text.strip()\n",
        "\n",
        "                data.append([url, f\"{section_title} - Step {index}\", prompt, response])\n",
        "\n",
        "    return data\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    all_data = []\n",
        "    for data in executor.map(scrape_page, urls):\n",
        "        all_data.extend(data)\n",
        "\n",
        "csv_filename = 'dataset_combined.csv'\n",
        "with open(csv_filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    csv_writer.writerow([\"Website Link\", \"Step Number\", \"Prompt\", \"Response\"])\n",
        "\n",
        "    with lock:\n",
        "        csv_writer.writerows(all_data)\n"
      ]
    }
  ]
}